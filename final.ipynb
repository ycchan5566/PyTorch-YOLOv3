{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=1, class_path='/home/dllab/final/clothes.names', conf_thres=0.8, config_path='/home/dllab/final/yolov3.cfg', image_folder='/home/dllab/clothes/clothes_test/images', img_size=640, n_cpu=8, nms_thres=0.4, use_cuda=True, weights_path='/tmp/work/50o.weights')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing object detection:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "/opt/conda/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "/opt/conda/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "/opt/conda/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "/opt/conda/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "/opt/conda/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "/opt/conda/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "/opt/conda/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:122: UserWarning: nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t+ Batch 0, Inference Time: 0:00:01.622500\n",
      "\t+ Batch 1, Inference Time: 0:00:00.024848\n",
      "\t+ Batch 2, Inference Time: 0:00:00.096508\n",
      "\t+ Batch 3, Inference Time: 0:00:00.019072\n",
      "\t+ Batch 4, Inference Time: 0:00:00.082695\n",
      "\t+ Batch 5, Inference Time: 0:00:00.102200\n",
      "\t+ Batch 6, Inference Time: 0:00:00.101745\n",
      "\t+ Batch 7, Inference Time: 0:00:00.084437\n",
      "\t+ Batch 8, Inference Time: 0:00:00.093469\n",
      "\t+ Batch 9, Inference Time: 0:00:00.023275\n",
      "\t+ Batch 10, Inference Time: 0:00:00.079765\n",
      "\t+ Batch 11, Inference Time: 0:00:00.024764\n",
      "\t+ Batch 12, Inference Time: 0:00:00.092807\n",
      "\t+ Batch 13, Inference Time: 0:00:00.097957\n",
      "\t+ Batch 14, Inference Time: 0:00:00.087259\n",
      "\t+ Batch 15, Inference Time: 0:00:00.024416\n",
      "\t+ Batch 16, Inference Time: 0:00:00.083107\n",
      "\t+ Batch 17, Inference Time: 0:00:00.081064\n",
      "\t+ Batch 18, Inference Time: 0:00:00.026518\n",
      "\t+ Batch 19, Inference Time: 0:00:00.090341\n",
      "\t+ Batch 20, Inference Time: 0:00:00.088102\n",
      "\t+ Batch 21, Inference Time: 0:00:00.026525\n",
      "\t+ Batch 22, Inference Time: 0:00:00.098616\n",
      "\t+ Batch 23, Inference Time: 0:00:00.096481\n",
      "\t+ Batch 24, Inference Time: 0:00:00.092409\n",
      "\t+ Batch 25, Inference Time: 0:00:00.023179\n",
      "\t+ Batch 26, Inference Time: 0:00:00.081778\n",
      "\t+ Batch 27, Inference Time: 0:00:00.088418\n",
      "\t+ Batch 28, Inference Time: 0:00:00.025355\n",
      "\t+ Batch 29, Inference Time: 0:00:00.089396\n",
      "\t+ Batch 30, Inference Time: 0:00:00.090300\n",
      "\t+ Batch 31, Inference Time: 0:00:00.023776\n",
      "\t+ Batch 32, Inference Time: 0:00:00.090382\n",
      "\t+ Batch 33, Inference Time: 0:00:00.085221\n",
      "\t+ Batch 34, Inference Time: 0:00:00.023475\n",
      "\t+ Batch 35, Inference Time: 0:00:00.089252\n",
      "\t+ Batch 36, Inference Time: 0:00:00.080624\n",
      "\t+ Batch 37, Inference Time: 0:00:00.027310\n",
      "\t+ Batch 38, Inference Time: 0:00:00.088361\n",
      "\t+ Batch 39, Inference Time: 0:00:00.078762\n",
      "\t+ Batch 40, Inference Time: 0:00:00.029006\n",
      "\t+ Batch 41, Inference Time: 0:00:00.097368\n",
      "\t+ Batch 42, Inference Time: 0:00:00.084535\n",
      "\t+ Batch 43, Inference Time: 0:00:00.024166\n",
      "\t+ Batch 44, Inference Time: 0:00:00.085496\n",
      "\t+ Batch 45, Inference Time: 0:00:00.025474\n",
      "\t+ Batch 46, Inference Time: 0:00:00.079211\n",
      "\t+ Batch 47, Inference Time: 0:00:00.084686\n",
      "\t+ Batch 48, Inference Time: 0:00:00.024026\n",
      "\t+ Batch 49, Inference Time: 0:00:00.084118\n",
      "\t+ Batch 50, Inference Time: 0:00:00.093598\n",
      "\t+ Batch 51, Inference Time: 0:00:00.087029\n",
      "\t+ Batch 52, Inference Time: 0:00:00.031696\n",
      "\t+ Batch 53, Inference Time: 0:00:00.085911\n",
      "\t+ Batch 54, Inference Time: 0:00:00.080535\n",
      "\t+ Batch 55, Inference Time: 0:00:00.028246\n",
      "\t+ Batch 56, Inference Time: 0:00:00.080256\n",
      "\t+ Batch 57, Inference Time: 0:00:00.026094\n",
      "\t+ Batch 58, Inference Time: 0:00:00.080049\n",
      "\t+ Batch 59, Inference Time: 0:00:00.024625\n",
      "\t+ Batch 60, Inference Time: 0:00:00.085785\n",
      "\t+ Batch 61, Inference Time: 0:00:00.089160\n",
      "\t+ Batch 62, Inference Time: 0:00:00.024537\n",
      "\t+ Batch 63, Inference Time: 0:00:00.086385\n",
      "\t+ Batch 64, Inference Time: 0:00:00.090877\n",
      "\t+ Batch 65, Inference Time: 0:00:00.024872\n",
      "\t+ Batch 66, Inference Time: 0:00:00.083432\n",
      "\t+ Batch 67, Inference Time: 0:00:00.023989\n",
      "\t+ Batch 68, Inference Time: 0:00:00.080875\n",
      "\t+ Batch 69, Inference Time: 0:00:00.096552\n",
      "\t+ Batch 70, Inference Time: 0:00:00.078867\n",
      "\t+ Batch 71, Inference Time: 0:00:00.027914\n",
      "\t+ Batch 72, Inference Time: 0:00:00.081178\n",
      "\t+ Batch 73, Inference Time: 0:00:00.023560\n",
      "\t+ Batch 74, Inference Time: 0:00:00.081039\n",
      "\t+ Batch 75, Inference Time: 0:00:00.080506\n",
      "\t+ Batch 76, Inference Time: 0:00:00.023839\n",
      "\t+ Batch 77, Inference Time: 0:00:00.088081\n",
      "\t+ Batch 78, Inference Time: 0:00:00.020795\n",
      "\t+ Batch 79, Inference Time: 0:00:00.082115\n",
      "\t+ Batch 80, Inference Time: 0:00:00.020058\n",
      "\t+ Batch 81, Inference Time: 0:00:00.088710\n",
      "\t+ Batch 82, Inference Time: 0:00:00.089552\n",
      "\t+ Batch 83, Inference Time: 0:00:00.019884\n",
      "\t+ Batch 84, Inference Time: 0:00:00.079268\n",
      "\t+ Batch 85, Inference Time: 0:00:00.020441\n",
      "\t+ Batch 86, Inference Time: 0:00:00.073652\n",
      "\t+ Batch 87, Inference Time: 0:00:00.022528\n",
      "\t+ Batch 88, Inference Time: 0:00:00.020091\n",
      "\t+ Batch 89, Inference Time: 0:00:00.018210\n",
      "\t+ Batch 90, Inference Time: 0:00:00.018811\n",
      "\t+ Batch 91, Inference Time: 0:00:00.017047\n",
      "\t+ Batch 92, Inference Time: 0:00:00.018389\n",
      "\t+ Batch 93, Inference Time: 0:00:00.018447\n",
      "\t+ Batch 94, Inference Time: 0:00:00.018301\n",
      "\t+ Batch 95, Inference Time: 0:00:00.018128\n",
      "\t+ Batch 96, Inference Time: 0:00:00.018368\n",
      "\t+ Batch 97, Inference Time: 0:00:00.018092\n",
      "\t+ Batch 98, Inference Time: 0:00:00.017948\n",
      "\t+ Batch 99, Inference Time: 0:00:00.018083\n",
      "\n",
      "Saving images:\n",
      "(0) Image: '/home/dllab/clothes/clothes_test/images/2001.jpg'\n",
      "(1) Image: '/home/dllab/clothes/clothes_test/images/2002.jpg'\n",
      "(2) Image: '/home/dllab/clothes/clothes_test/images/2003.jpg'\n",
      "(3) Image: '/home/dllab/clothes/clothes_test/images/2004.jpg'\n",
      "(4) Image: '/home/dllab/clothes/clothes_test/images/2005.jpg'\n",
      "(5) Image: '/home/dllab/clothes/clothes_test/images/2006.jpg'\n",
      "(6) Image: '/home/dllab/clothes/clothes_test/images/2007.jpg'\n",
      "(7) Image: '/home/dllab/clothes/clothes_test/images/2008.jpg'\n",
      "(8) Image: '/home/dllab/clothes/clothes_test/images/2009.jpg'\n",
      "(9) Image: '/home/dllab/clothes/clothes_test/images/2010.jpg'\n",
      "(10) Image: '/home/dllab/clothes/clothes_test/images/2011.jpg'\n",
      "(11) Image: '/home/dllab/clothes/clothes_test/images/2012.jpg'\n",
      "(12) Image: '/home/dllab/clothes/clothes_test/images/2013.jpg'\n",
      "(13) Image: '/home/dllab/clothes/clothes_test/images/2014.jpg'\n",
      "(14) Image: '/home/dllab/clothes/clothes_test/images/2015.jpg'\n",
      "(15) Image: '/home/dllab/clothes/clothes_test/images/2016.jpg'\n",
      "(16) Image: '/home/dllab/clothes/clothes_test/images/2017.jpg'\n",
      "(17) Image: '/home/dllab/clothes/clothes_test/images/2018.jpg'\n",
      "(18) Image: '/home/dllab/clothes/clothes_test/images/2019.jpg'\n",
      "(19) Image: '/home/dllab/clothes/clothes_test/images/2020.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/matplotlib/pyplot.py:537: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20) Image: '/home/dllab/clothes/clothes_test/images/2021.jpg'\n",
      "(21) Image: '/home/dllab/clothes/clothes_test/images/2022.jpg'\n",
      "(22) Image: '/home/dllab/clothes/clothes_test/images/2023.jpg'\n",
      "(23) Image: '/home/dllab/clothes/clothes_test/images/2024.jpg'\n",
      "(24) Image: '/home/dllab/clothes/clothes_test/images/2025.jpg'\n",
      "(25) Image: '/home/dllab/clothes/clothes_test/images/2026.jpg'\n",
      "(26) Image: '/home/dllab/clothes/clothes_test/images/2027.jpg'\n",
      "(27) Image: '/home/dllab/clothes/clothes_test/images/2028.jpg'\n",
      "(28) Image: '/home/dllab/clothes/clothes_test/images/2029.jpg'\n",
      "(29) Image: '/home/dllab/clothes/clothes_test/images/2030.jpg'\n",
      "(30) Image: '/home/dllab/clothes/clothes_test/images/2031.jpg'\n",
      "(31) Image: '/home/dllab/clothes/clothes_test/images/2032.jpg'\n",
      "(32) Image: '/home/dllab/clothes/clothes_test/images/2033.jpg'\n",
      "(33) Image: '/home/dllab/clothes/clothes_test/images/2034.jpg'\n",
      "(34) Image: '/home/dllab/clothes/clothes_test/images/2035.jpg'\n",
      "(35) Image: '/home/dllab/clothes/clothes_test/images/2036.jpg'\n",
      "(36) Image: '/home/dllab/clothes/clothes_test/images/2037.jpg'\n",
      "(37) Image: '/home/dllab/clothes/clothes_test/images/2038.jpg'\n",
      "(38) Image: '/home/dllab/clothes/clothes_test/images/2039.jpg'\n",
      "(39) Image: '/home/dllab/clothes/clothes_test/images/2040.jpg'\n",
      "(40) Image: '/home/dllab/clothes/clothes_test/images/2041.jpg'\n",
      "(41) Image: '/home/dllab/clothes/clothes_test/images/2042.jpg'\n",
      "(42) Image: '/home/dllab/clothes/clothes_test/images/2043.jpg'\n",
      "(43) Image: '/home/dllab/clothes/clothes_test/images/2044.jpg'\n",
      "(44) Image: '/home/dllab/clothes/clothes_test/images/2045.jpg'\n",
      "(45) Image: '/home/dllab/clothes/clothes_test/images/2046.jpg'\n",
      "(46) Image: '/home/dllab/clothes/clothes_test/images/2047.jpg'\n",
      "(47) Image: '/home/dllab/clothes/clothes_test/images/2048.jpg'\n",
      "(48) Image: '/home/dllab/clothes/clothes_test/images/2049.jpg'\n",
      "(49) Image: '/home/dllab/clothes/clothes_test/images/2050.jpg'\n",
      "(50) Image: '/home/dllab/clothes/clothes_test/images/2051.jpg'\n",
      "(51) Image: '/home/dllab/clothes/clothes_test/images/2052.jpg'\n",
      "(52) Image: '/home/dllab/clothes/clothes_test/images/2053.jpg'\n",
      "(53) Image: '/home/dllab/clothes/clothes_test/images/2054.jpg'\n",
      "(54) Image: '/home/dllab/clothes/clothes_test/images/2055.jpg'\n",
      "(55) Image: '/home/dllab/clothes/clothes_test/images/2056.jpg'\n",
      "(56) Image: '/home/dllab/clothes/clothes_test/images/2057.jpg'\n",
      "(57) Image: '/home/dllab/clothes/clothes_test/images/2058.jpg'\n",
      "(58) Image: '/home/dllab/clothes/clothes_test/images/2059.jpg'\n",
      "(59) Image: '/home/dllab/clothes/clothes_test/images/2060.jpg'\n",
      "(60) Image: '/home/dllab/clothes/clothes_test/images/2061.jpg'\n",
      "(61) Image: '/home/dllab/clothes/clothes_test/images/2062.jpg'\n",
      "(62) Image: '/home/dllab/clothes/clothes_test/images/2063.jpg'\n",
      "(63) Image: '/home/dllab/clothes/clothes_test/images/2064.jpg'\n",
      "(64) Image: '/home/dllab/clothes/clothes_test/images/2065.jpg'\n",
      "(65) Image: '/home/dllab/clothes/clothes_test/images/2066.jpg'\n",
      "(66) Image: '/home/dllab/clothes/clothes_test/images/2067.jpg'\n",
      "(67) Image: '/home/dllab/clothes/clothes_test/images/2068.jpg'\n",
      "(68) Image: '/home/dllab/clothes/clothes_test/images/2069.jpg'\n",
      "(69) Image: '/home/dllab/clothes/clothes_test/images/2070.jpg'\n",
      "(70) Image: '/home/dllab/clothes/clothes_test/images/2071.jpg'\n",
      "(71) Image: '/home/dllab/clothes/clothes_test/images/2072.jpg'\n",
      "(72) Image: '/home/dllab/clothes/clothes_test/images/2073.jpg'\n",
      "(73) Image: '/home/dllab/clothes/clothes_test/images/2074.jpg'\n",
      "(74) Image: '/home/dllab/clothes/clothes_test/images/2075.jpg'\n",
      "(75) Image: '/home/dllab/clothes/clothes_test/images/2076.jpg'\n",
      "(76) Image: '/home/dllab/clothes/clothes_test/images/2077.jpg'\n",
      "(77) Image: '/home/dllab/clothes/clothes_test/images/2078.jpg'\n",
      "(78) Image: '/home/dllab/clothes/clothes_test/images/2079.jpg'\n",
      "(79) Image: '/home/dllab/clothes/clothes_test/images/2080.jpg'\n",
      "(80) Image: '/home/dllab/clothes/clothes_test/images/2081.jpg'\n",
      "(81) Image: '/home/dllab/clothes/clothes_test/images/2082.jpg'\n",
      "(82) Image: '/home/dllab/clothes/clothes_test/images/2083.jpg'\n",
      "(83) Image: '/home/dllab/clothes/clothes_test/images/2084.jpg'\n",
      "(84) Image: '/home/dllab/clothes/clothes_test/images/2085.jpg'\n",
      "(85) Image: '/home/dllab/clothes/clothes_test/images/2086.jpg'\n",
      "(86) Image: '/home/dllab/clothes/clothes_test/images/2087.jpg'\n",
      "(87) Image: '/home/dllab/clothes/clothes_test/images/2088.jpg'\n",
      "(88) Image: '/home/dllab/clothes/clothes_test/images/2089.jpg'\n",
      "(89) Image: '/home/dllab/clothes/clothes_test/images/2090.jpg'\n",
      "(90) Image: '/home/dllab/clothes/clothes_test/images/2091.jpg'\n",
      "(91) Image: '/home/dllab/clothes/clothes_test/images/2092.jpg'\n",
      "(92) Image: '/home/dllab/clothes/clothes_test/images/2093.jpg'\n",
      "(93) Image: '/home/dllab/clothes/clothes_test/images/2094.jpg'\n",
      "(94) Image: '/home/dllab/clothes/clothes_test/images/2095.jpg'\n",
      "(95) Image: '/home/dllab/clothes/clothes_test/images/2096.jpg'\n",
      "(96) Image: '/home/dllab/clothes/clothes_test/images/2097.jpg'\n",
      "(97) Image: '/home/dllab/clothes/clothes_test/images/2098.jpg'\n",
      "(98) Image: '/home/dllab/clothes/clothes_test/images/2099.jpg'\n",
      "(99) Image: '/home/dllab/clothes/clothes_test/images/2100.jpg'\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from models import *\n",
    "from utils.utils import *\n",
    "from utils.datasets import *\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.ticker import NullLocator\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--image_folder', type=str, default='data/samples', help='path to dataset')\n",
    "parser.add_argument('--config_path', type=str, default='config/yolov3.cfg', help='path to model config file')\n",
    "parser.add_argument('--weights_path', type=str, default='weights/yolov3.weights', help='path to weights file')\n",
    "parser.add_argument('--class_path', type=str, default='data/coco.names', help='path to class label file')\n",
    "parser.add_argument('--conf_thres', type=float, default=0.8, help='object confidence threshold')\n",
    "parser.add_argument('--nms_thres', type=float, default=0.4, help='iou thresshold for non-maximum suppression')\n",
    "parser.add_argument('--batch_size', type=int, default=1, help='size of the batches')\n",
    "parser.add_argument('--n_cpu', type=int, default=8, help='number of cpu threads to use during batch generation')\n",
    "parser.add_argument('--img_size', type=int, default=416, help='size of each image dimension')\n",
    "parser.add_argument('--use_cuda', type=bool, default=True, help='whether to use cuda if available')\n",
    "opt = parser.parse_args(args=['--config_path', '/home/dllab/final/yolov3.cfg', '--class_path', '/home/dllab/final/clothes.names', '--image_folder', '/home/dllab/clothes/clothes_test/images', '--weights_path', '/tmp/work/50o.weights', '--img_size', '640'])\n",
    "print(opt)\n",
    "\n",
    "cuda = torch.cuda.is_available() and opt.use_cuda\n",
    "\n",
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "# Set up model\n",
    "model = Darknet(opt.config_path, img_size=opt.img_size)\n",
    "model.load_weights(opt.weights_path)\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "model.eval() # Set in evaluation mode\n",
    "\n",
    "dataloader = DataLoader(ImageFolder(opt.image_folder, img_size=opt.img_size),\n",
    "                        batch_size=opt.batch_size, shuffle=False, num_workers=opt.n_cpu)\n",
    "\n",
    "classes = load_classes(opt.class_path) # Extracts class labels from file\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "imgs = []           # Stores image paths\n",
    "img_detections = [] # Stores detections for each image index\n",
    "\n",
    "print ('\\nPerforming object detection:')\n",
    "prev_time = time.time()\n",
    "for batch_i, (img_paths, input_imgs) in enumerate(dataloader):\n",
    "    # Configure input\n",
    "    input_imgs = Variable(input_imgs.type(Tensor))\n",
    "\n",
    "    # Get detections\n",
    "    with torch.no_grad():\n",
    "        detections = model(input_imgs)\n",
    "        detections = non_max_suppression(detections, 80, opt.conf_thres, opt.nms_thres)\n",
    "\n",
    "\n",
    "    # Log progress\n",
    "    current_time = time.time()\n",
    "    inference_time = datetime.timedelta(seconds=current_time - prev_time)\n",
    "    prev_time = current_time\n",
    "    print ('\\t+ Batch %d, Inference Time: %s' % (batch_i, inference_time))\n",
    "\n",
    "    # Save image and detections\n",
    "    imgs.extend(img_paths)\n",
    "    img_detections.extend(detections)\n",
    "\n",
    "# Bounding-box colors\n",
    "cmap = plt.get_cmap('tab20b')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, 20)]\n",
    "\n",
    "print ('\\nSaving images:')\n",
    "# Iterate through images and save plot of detections\n",
    "for img_i, (path, detections) in enumerate(zip(imgs, img_detections)):\n",
    "\n",
    "    print (\"(%d) Image: '%s'\" % (img_i, path))\n",
    "\n",
    "    # Create plot\n",
    "    img = np.array(Image.open(path))\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(img)\n",
    "\n",
    "    # The amount of padding that was added\n",
    "    pad_x = max(img.shape[0] - img.shape[1], 0) * (opt.img_size / max(img.shape))\n",
    "    pad_y = max(img.shape[1] - img.shape[0], 0) * (opt.img_size / max(img.shape))\n",
    "    # Image height and width after padding is removed\n",
    "    unpad_h = opt.img_size - pad_y\n",
    "    unpad_w = opt.img_size - pad_x\n",
    "\n",
    "    # Draw bounding boxes and labels of detections\n",
    "    if detections is not None:\n",
    "        unique_labels = detections[:, -1].cpu().unique()\n",
    "        n_cls_preds = len(unique_labels)\n",
    "        bbox_colors = random.sample(colors, n_cls_preds)\n",
    "        for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections:\n",
    "\n",
    "            print ('\\t+ Label: %s, Conf: %.5f' % (classes[int(cls_pred)], cls_conf.item()))\n",
    "\n",
    "            # Rescale coordinates to original dimensions\n",
    "            box_h = ((y2 - y1) / unpad_h) * img.shape[0]\n",
    "            box_w = ((x2 - x1) / unpad_w) * img.shape[1]\n",
    "            y1 = ((y1 - pad_y // 2) / unpad_h) * img.shape[0]\n",
    "            x1 = ((x1 - pad_x // 2) / unpad_w) * img.shape[1]\n",
    "\n",
    "            color = bbox_colors[int(np.where(unique_labels == int(cls_pred))[0])]\n",
    "            # Create a Rectangle patch\n",
    "            bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=2,\n",
    "                                    edgecolor=color,\n",
    "                                    facecolor='none')\n",
    "            # Add the bbox to the plot\n",
    "            ax.add_patch(bbox)\n",
    "            # Add label\n",
    "            plt.text(x1, y1, s=classes[int(cls_pred)], color='white', verticalalignment='top',\n",
    "                    bbox={'color': color, 'pad': 0})\n",
    "\n",
    "    # Save generated image with detections\n",
    "    plt.axis('off')\n",
    "    plt.gca().xaxis.set_major_locator(NullLocator())\n",
    "    plt.gca().yaxis.set_major_locator(NullLocator())\n",
    "    plt.savefig('output/%d.png' % (img_i), bbox_inches='tight', pad_inches=0.0)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
